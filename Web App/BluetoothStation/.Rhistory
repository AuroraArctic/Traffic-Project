fat.preds = predict(fat.model, fat.test)
summary(fat.model)
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ .-siri-density"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ .-siri-density"))
#MSE Least Squares
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ abdomen+wrist+thigh+age"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ abdomen+wrist+thigh+age"))
#MSE Least Squares
fat.model = lm(y ~ abdomen+wrist+thigh+age,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ .-siri-density-abdomen"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ .-siri-density-abdomen"))
#MSE Least Squares
fat.model = lm(y ~.-siri-density-abdomen,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Data import
library(ISLR)
hit = na.omit(Hitters) # Removing missing information
# Train-test split
set.seed(10)
h.train = sample(nrow(hit), 0.5*nrow(hit))
hit.test = hit[-h.train,]
hit.train= hit[-h.train,]
# Necessary for MSE
hit.y.test = hit[-h.train,"Salary"]
# Necessary for predicting the outcome for the test set in RandomForest
hit.x.test = hit[-h.train,-which(names(hit) %in% c("Salary"))]
mses = data.frame(matrix(ncol=2,nrow=0, dimnames=list(NULL, c("Tree","MSE"))))
if(!require(tree)) {
install.packages("tree")
library(tree)
}
# Creating and fitting the decision tree on the training data
tree.hit <- tree(Salary ~ ., data=hit.train)
# Predicting the outcome for testing data
tree.pred <- predict(tree.hit, newdata=hit.test)
# Computing MSE on test data
mses[nrow(mses)+1,] = c("Unpruned Decision Tree",       # Name of the model tree
mean((tree.pred-hit.y.test)^2)) # MSE on test data
mses[1,]
# Plot decision tree
plot(tree.hit)
text(tree.hit, pretty=0, cex=0.6)
set.seed(20)
#Cross validation for tree complexity (i.e. number of terminal nodes)
cv.hit <- cv.tree(tree.hit)
# Plot deviance vs size
plot(cv.hit$size,cv.hit$dev,type='b',main="Deviance depending on the size of the tree",xlab="Size",ylab="Deviance")
points(cv.hit$size[which.min(cv.hit$dev)],min(cv.hit$dev),col='red',cex=1.5,pch=15)
abline(v= cv.hit$size[which.min(cv.hit$dev)],col="red",lty=2)
# Saving the size that returns the minimum deviance
best.size = cv.hit$size[which.min(cv.hit$dev)]
# Optimal Number of Nodes
prune.hit = prune.tree(tree.hit,best=best.size)
plot(prune.hit)
text(prune.hit, pretty=0,cex=0.7)
tree.pred.opt = predict(prune.hit,newdata=hit.test)
mses[nrow(mses)+1,] = c("Pruned Decision Tree", mean((tree.pred.opt-hit.y.test)^2)) # MSE on test data
mses
library(randomForest)
set.seed(11)
n_pred <- ncol(hit) - 1
# Fitting the tree and predicting test values
bag.hit <- randomForest(Salary ~ .,
data=hit.train,
mtry=n_pred,
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
yhat.bag <- bag.hit$test$predicted
# MSE on test data
mses[nrow(mses)+1,] = c("Bagging",mean((yhat.bag - hit.y.test)^2))
mses
varImpPlot(bag.hit)
m.seq = 1:n_pred # Range of mtry possible values
# Matrix where MSE and OOB error are saved
m_df = matrix(nrow=length(m.seq),ncol=2)
# Cross validation on mtry parameter
for(m in m.seq){
set.seed(11) # Necessary, or results will change after every execution
model <- randomForest(Salary ~ .,
data=hit.train,
mtry=m,
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
m_df[m,2] = mean(model$test$mse) # OOB error
m_df[m,1] = mean((hit.y.test-model$test$predicted)^2) # test MSE
}
# Optimal parameters
message("mtry based on the MSE: ",which.min(m_df[,1])) # based on MSE
message("mtry based on the OOB: ",which.min(m_df[,2])) # based on OOB error
plot(m.seq,m_df[,1],col='#7c4cff',type='b',pch=19,
main="Number of predictors vs test error",
xlab="Number of predictors (m)",
ylab="test MSE or OOB error")
lines(m.seq,m_df[,2],col='#ffbd4c',type='b',pch=18)
abline(v= which.min(m_df[,2]),col="grey",lty=2)
abline(v= which.min(m_df[,1]),col="grey",lty=2)
points(which.min(m_df[,2]),min(m_df[,2]),col='#ffbd4c',cex=1.5,pch=15)
points(which.min(m_df[,1]),min(m_df[,1]),col='#7c4cff',cex=1.5,pch=15)
legend(5,35000,
legend = c("MSE","OOB error"),
col=c("#7c4cff","#ffbd4c"),
lty=1)
set.seed(11) # Necessary, or results will change after every execution
best.rf.model <- randomForest(Salary ~ .,
data=hit.train,
mtry=which.min(m_df[,1]),  # choosing the m with minimum MSE
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
# test MSE
mses[nrow(mses)+1,] = c("Random Forest", mean((hit.y.test-best.rf.model$test$predicted)^2))
mses
# Inspecting importance of predictors
varImpPlot(best.rf.model)
if (!require("gbm")){
install.packages("gbm")
library(gbm)
}
# Fitting the boosting model for regression
set.seed(20)
boosted <- gbm(Salary ~ .,
data=hit.train,
distribution="gaussian",
n.trees=2000,
cv.folds = 5)
# Boosting MSE
mses[nrow(mses)+1,] = c("Boosting",
mean((predict(boosted, hit.test)-hit.y.test)^2))
mses
# Plot with CV and OOB error against the number of iterations
par(mfrow=c(1,2))
boosted.cv = gbm.perf(boosted, method = "cv")
boosted.oob = gbm.perf(boosted, method = "OOB")
# Try optimal solution of OOB
set.seed(20)
boosted.oob.model = gbm(Salary ~ .,
data=hit.train,
distribution="gaussian",
n.trees=boosted.oob,
cv.folds = 0)
preds.boosted.oob = predict(boosted.oob.model, hit.test)
# Try optimal solution of cv
set.seed(20)
boosted.cv.model = gbm(Salary ~ .,
data=hit.train,
distribution="gaussian",
n.trees=boosted.cv,
cv.folds = 0)
preds.boosted.cv = predict(boosted.cv.model, hit.test)
mses[nrow(mses)+1,] = c("OOB Optimized Boosting Tree",
mean((preds.boosted.oob-hit.y.test)^2))
mses[nrow(mses)+1,] = c("CV Optimized Boosting Tree",
mean((preds.boosted.cv-hit.y.test)^2))
mses
# Partial dependence Plot about Boosting
par(mfrow=c(2,2))
plot(boosted.cv.model, i="Years")
plot(boosted.cv.model, i="CAtBat")
plot(boosted.cv.model, i="Errors")
plot(boosted.cv.model, i="CHmRun")
mses$MSE = as.numeric(mses$MSE)
mses[order(mses$MSE),]
coef_comp
knitr::kable(head(fat), format = "latex")
fat = read.csv("fat.tsv",sep="\t")
knitr::kable(head(fat), format = "latex")
install.packages("kableExtra")
library(kableExtra)
head(fat) %>%
kbl() %>%
kable_styling()
# Partial dependence Plot about Boosting
install.packages("gridExtra")
library(gridExtra)
grid.arrange(plot(boosted.cv.model, i="Years"),
plot(boosted.cv.model, i="CAtBat"),
plot(boosted.cv.model, i="Errors"),
plot(boosted.cv.model, i="CHmRun"))
install.packages("ghostcript")
boosted.oob = gbm.perf(boosted, method = "OOB")
# Necessary for obtaining the same output in RMarkdown and PDF
RNGkind(sample.kind="Rounding")
if(!require("kableExtra")){
install.packages("kableExtra")
library(kableExtra)
}
fat = read.csv("fat.tsv",sep="\t")
summary(fat)
hist(fat$y,breaks = 45,freq=FALSE,
main = "Distribution of the outcome variable y",
xlab='y (body fat percentage)',
ylab='Density')
lines(density(fat$y))
library(corrplot)
cor = cor(fat)
col<- colorRampPalette(c("red", "white", "blue"))(100)
corrplot(cor,
method='square',
type='lower',
order="hclust",
diag=FALSE,
tl.srt=15,
col=col,
title = "Correlation matrix of the fat dataset",
mar=c(0,0,2,0))
set.seed(1)
f.train = sample(nrow(fat), 0.5*nrow(fat))
fat.test = fat[-f.train,]
fat.train= fat[f.train,]
fat.y.train = fat[f.train,"y"]
fat.y.test = fat[-f.train,"y"]
# Creating model matrices
x = model.matrix(y ~ ., data=fat)[, -1]
y = fat$y
x_train <- model.matrix(y ~ ., fat.train)[, -1]
x_test <- model.matrix(y ~ ., fat.test)[, -1]
fat.model = lm(y ~.,
data=fat,
subset = f.train)
summary(fat.model)
fat.preds = predict(fat.model, fat.test)
#MSE
mean((fat.preds-fat.y.test)^2)
library(glmnet)
# Fitting the Ridge Regression
lambdas = 10^seq(5,-3, length.out = 1000)
fat.ridge = glmnet(x_train,fat.y.train,
alpha=0, lambda=lambdas,exact=TRUE)
# Plot coefficients as a function of lambda
plot(fat.ridge, xvar="lambda")
# Cross validation
cv.ridge = cv.glmnet(x_test,
fat.y.test,
alpha=0,
lambda = fat.ridge$lambda)
# Plot CV of MSE as a function of Lambda
plot(cv.ridge)
# Best lambda (with minimum MSE)
best_lambda = cv.ridge$lambda.min
# Computing MSE with the selected Lambda
fat.ridge = glmnet(x_train,
fat.y.train,
alpha=0,
lambda = best_lambda)
ridge_pred <- predict(fat.ridge, newx=x_test)
# Test MSE
mean((ridge_pred - fat.y.test)^2)
# Examining the coefficients
coef_comp = cbind(as.data.frame(coef(fat.model)),
as.data.frame(summary(coef(fat.ridge))$x))
colnames(coef_comp) = c("Least Squares","Ridge")
coef_comp$`Smaller coefficients` = ifelse(coef_comp$Ridge< coef_comp$`Least Squares`,TRUE,FALSE)
coef_comp$`Least Squares` = round(coef_comp$`Least Squares`,5)
coef_comp$Ridge = round(coef_comp$Ridge,5)
coef_comp %>%
kbl() %>%
kable_styling()
# Fitting the Lasso Regression
fat.lasso = glmnet(x_train,fat.y.train, alpha=1,lambda = lambdas)
# Plot coefficients as a function of lambda
plot(fat.lasso, xvar="lambda")
# Cross validation
cv.lasso = cv.glmnet(x_test,
fat.y.test,
alpha=1,
lambda = fat.lasso$lambda)
# Plot CV of MSE as a function of Lambda
plot(cv.lasso)
# Best lambda (with minimum MSE)
best_lambda = cv.lasso$lambda.min
# Computing MSE with the selected Lambda
fat.lasso = glmnet(x_train,
fat.y.train,
alpha=1,
lambda = best_lambda)
lasso_pred <- predict(fat.lasso, newx=x_test)
# Test MSE
mean((lasso_pred - fat.y.test)^2)
# Examining the coefficients
coef(fat.lasso)
perform_regression = function(alpha, formula, lambda=lambdas){
formula = as.formula(formula)
# Creating a new model matrix based on the given predictors
x_train = model.matrix(formula, fat.train)[, -1]
x_test = model.matrix(formula, fat.test)[, -1]
# Cross validation for lambda
cv.model = cv.glmnet(x_test,
fat.y.test,
alpha=alpha,
lambda = fat.model$lambdas)
# Best lambda (with minimum MSE)
best_lambda = cv.model$lambda.min
# Computing MSE with the selected Lambda
fat.model = glmnet(x_train,
fat.y.train,
alpha=alpha,
lambda = best_lambda)
preds <- predict(fat.model, newx=x_test)
return(mean((preds - fat.y.test)^2))
}
mses = data.frame(matrix(ncol=3,nrow=0,
dimnames=list(NULL, c("Regression",
"Formula",
"MSE"))))
formulae = c("y ~ .-density-siri", # no density and siri
"y ~ .-density", # no density
"y ~ .-siri", # no siri
"y ~ .") # all predictors
for(alpha in c(0,1)){
for(formula in formulae){
if(alpha==0){
mses[nrow(mses)+1,] = c("Ridge",formula,perform_regression(alpha=alpha,formula))
}else{
mses[nrow(mses)+1,] = c("Lasso",formula,perform_regression(alpha=alpha,formula))
}
}
}
mses$MSE = as.numeric(mses$MSE)
mses[order(mses$MSE),] %>%
kbl() %>%
kable_styling()
fat.model = lm(y ~.-density,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
#MSE
mean((fat.preds-fat.y.test)^2)
summary(fat.model)
# Least squares
fat.model = lm(y ~.-siri-density,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
summary(fat.model)
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ .-siri-density"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ .-siri-density"))
#MSE Least Squares
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ abdomen+wrist+thigh+age"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ abdomen+wrist+thigh+age"))
#MSE Least Squares
fat.model = lm(y ~ abdomen+wrist+thigh+age,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Ridge
message("Ridge: ",perform_regression(alpha=0,"y ~ .-siri-density-abdomen"))
# Lasso
message("Lasso: ",perform_regression(alpha=1,"y ~ .-siri-density-abdomen"))
#MSE Least Squares
fat.model = lm(y ~.-siri-density-abdomen,
data=fat,
subset = f.train)
fat.preds = predict(fat.model, fat.test)
message("Least Squares: ",mean((fat.preds-fat.y.test)^2))
# Data import
library(ISLR)
hit = na.omit(Hitters) # Removing missing information
# Train-test split
set.seed(10)
h.train = sample(nrow(hit), 0.5*nrow(hit))
hit.test = hit[-h.train,]
hit.train= hit[-h.train,]
# Necessary for MSE
hit.y.test = hit[-h.train,"Salary"]
# Necessary for predicting the outcome for the test set in RandomForest
hit.x.test = hit[-h.train,-which(names(hit) %in% c("Salary"))]
mses = data.frame(matrix(ncol=2,nrow=0, dimnames=list(NULL, c("Tree","MSE"))))
if(!require(tree)) {
install.packages("tree")
library(tree)
}
# Creating and fitting the decision tree on the training data
tree.hit <- tree(Salary ~ ., data=hit.train)
# Predicting the outcome for testing data
tree.pred <- predict(tree.hit, newdata=hit.test)
# Computing MSE on test data
mses[nrow(mses)+1,] = c("Unpruned Decision Tree",       # Name of the model tree
mean((tree.pred-hit.y.test)^2)) # MSE on test data
mses[1,]
# Plot decision tree
plot(tree.hit)
text(tree.hit, pretty=0, cex=0.6)
set.seed(20)
#Cross validation for tree complexity (i.e. number of terminal nodes)
cv.hit <- cv.tree(tree.hit)
# Plot deviance vs size
plot(cv.hit$size,cv.hit$dev,type='b',main="Deviance depending on the size of the tree",xlab="Size",ylab="Deviance")
points(cv.hit$size[which.min(cv.hit$dev)],min(cv.hit$dev),col='red',cex=1.5,pch=15)
abline(v= cv.hit$size[which.min(cv.hit$dev)],col="red",lty=2)
# Saving the size that returns the minimum deviance
best.size = cv.hit$size[which.min(cv.hit$dev)]
# Optimal Number of Nodes
prune.hit = prune.tree(tree.hit,best=best.size)
plot(prune.hit)
text(prune.hit, pretty=0,cex=0.7)
tree.pred.opt = predict(prune.hit,newdata=hit.test)
mses[nrow(mses)+1,] = c("Pruned Decision Tree", mean((tree.pred.opt-hit.y.test)^2)) # MSE on test data
mses
library(randomForest)
set.seed(11)
n_pred <- ncol(hit) - 1
# Fitting the tree and predicting test values
bag.hit <- randomForest(Salary ~ .,
data=hit.train,
mtry=n_pred,
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
yhat.bag <- bag.hit$test$predicted
# MSE on test data
mses[nrow(mses)+1,] = c("Bagging",mean((yhat.bag - hit.y.test)^2))
mses
varImpPlot(bag.hit)
m.seq = 1:n_pred # Range of mtry possible values
# Matrix where MSE and OOB error are saved
m_df = matrix(nrow=length(m.seq),ncol=2)
# Cross validation on mtry parameter
for(m in m.seq){
set.seed(11) # Necessary, or results will change after every execution
model <- randomForest(Salary ~ .,
data=hit.train,
mtry=m,
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
m_df[m,2] = mean(model$test$mse) # OOB error
m_df[m,1] = mean((hit.y.test-model$test$predicted)^2) # test MSE
}
message("mtry based on the MSE: ",which.min(m_df[,1])) # based on MSE
message("mtry based on the OOB: ",which.min(m_df[,2])) # based on OOB error
plot(m.seq,m_df[,1],col='#7c4cff',type='b',pch=19,
main="Number of predictors vs test error",
xlab="Number of predictors (m)",
ylab="test MSE or OOB error")
lines(m.seq,m_df[,2],col='#ffbd4c',type='b',pch=18)
abline(v= which.min(m_df[,2]),col="grey",lty=2)
abline(v= which.min(m_df[,1]),col="grey",lty=2)
points(which.min(m_df[,2]),min(m_df[,2]),col='#ffbd4c',cex=1.5,pch=15)
points(which.min(m_df[,1]),min(m_df[,1]),col='#7c4cff',cex=1.5,pch=15)
legend(5,35000,
legend = c("MSE","OOB error"),
col=c("#7c4cff","#ffbd4c"),
lty=1)
set.seed(11) # Necessary, or results will change after every execution
best.rf.model <- randomForest(Salary ~ .,
data=hit.train,
mtry=which.min(m_df[,1]),  # choosing the m with minimum MSE
xtest = hit.x.test,
ytest = hit.y.test,
importance=TRUE)
# test MSE
mses[nrow(mses)+1,] = c("Random Forest",
mean((hit.y.test-best.rf.model$test$predicted)^2))
mses
# Inspecting importance of predictors
varImpPlot(best.rf.model)
if (!require("gbm")){
install.packages("gbm")
library(gbm)
}
# Fitting the boosting model for regression
set.seed(20)
boosted <- gbm(Salary ~ .,
data=hit.train,
distribution="gaussian",
n.trees=2000,
cv.folds = 5)
# Boosting MSE
mses[nrow(mses)+1,] = c("Boosting",
mean((predict(boosted, hit.test)-hit.y.test)^2))
mses
# Plot with CV and OOB error against the number of iterations
par(mfrow=c(1,2))
boosted.cv = gbm.perf(boosted, method = "cv")
boosted.oob = gbm.perf(boosted, method = "OOB")
#import data
getwd()
#import data
setwd("G:/Il mio Drive/First Year/Big Data Technologies/Traffic Project/")
data <- read.csv("data/BluetoothStations.csv")
data
data <- read.csv("data/BluetoothStations.csv")
data
data
data <- read.csv("data/BluetoothStations.csv",header = FALSE)
data
data <- read.csv("data/BluetoothStations.csv")
shiny::runApp('Web App/BluetoothStation')
runApp('Web App/BluetoothStation')
runApp('Web App/BluetoothStation')
